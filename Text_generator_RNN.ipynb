{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7jDkD18q8WF5DVegiG0Mc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FRED984/text_generator_with_RNN/blob/main/Text_generator_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zG7pmTky6kKy",
        "outputId": "8c477ddf-5eb2-4aad-8d4b-29816ac61631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwnQGc-Z7qa_",
        "outputId": "55ef6c4b-aeca-4f1d-b373-1cc080b229a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 22665,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 21631,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 19193,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 5244,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 10311,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 5952,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 31050,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 12118,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 7486,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 5535,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 5345,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "qwVmdKOh9OpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "dujDMIyl9k7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74i1GXcE-cpY",
        "outputId": "a1d19591-4c5e-4785-dd77-1fa2ff8e1579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2843041 (10.85 MB)\n",
            "Trainable params: 2843041 (10.85 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
        "\n",
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBKRgwor_MrT",
        "outputId": "5340530a-63dd-458a-f016-d2acbf2f5bb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 70s 108ms/step - loss: 0.4661 - acc: 0.7659 - val_loss: 0.3217 - val_acc: 0.8702\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 69s 111ms/step - loss: 0.2744 - acc: 0.8936 - val_loss: 0.2983 - val_acc: 0.8828\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 78s 124ms/step - loss: 0.2101 - acc: 0.9231 - val_loss: 0.2894 - val_acc: 0.8804\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 87s 139ms/step - loss: 0.1671 - acc: 0.9406 - val_loss: 0.3366 - val_acc: 0.8854\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 78s 125ms/step - loss: 0.1388 - acc: 0.9519 - val_loss: 0.3173 - val_acc: 0.8824\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 81s 130ms/step - loss: 0.1147 - acc: 0.9623 - val_loss: 0.3126 - val_acc: 0.8792\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 85s 136ms/step - loss: 0.0960 - acc: 0.9696 - val_loss: 0.3602 - val_acc: 0.8766\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 78s 125ms/step - loss: 0.0788 - acc: 0.9750 - val_loss: 0.4201 - val_acc: 0.8778\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 75s 121ms/step - loss: 0.0671 - acc: 0.9794 - val_loss: 0.4663 - val_acc: 0.8768\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 77s 123ms/step - loss: 0.0571 - acc: 0.9814 - val_loss: 0.4446 - val_acc: 0.8536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy4nRkl_BUOG",
        "outputId": "06eb8876-39ab-4c9b-a3fa-160a1cca06c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 20s 25ms/step - loss: 0.4705 - acc: 0.8498\n",
            "[0.4704665541648865, 0.8497599959373474]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"RNN.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzFzptOfDX79",
        "outputId": "015dcf1c-0a0e-409b-c39a-6818749ade67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = tf.keras.models.load_model(\"RNN.h5\")"
      ],
      "metadata": {
        "id": "aMrOKY3WD5sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making predictions"
      ],
      "metadata": {
        "id": "uTvQtbEfNsP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode_text(text):\n",
        "  tokens = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n",
        "  return sequence.pad_sequences([tokens], MAXLEN)[0]\n",
        "\n",
        "text = \"that movie was just amazing, so amazing\"\n",
        "encoded = encode_text(text)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDYi3GOKNqcJ",
        "outputId": "257ce3c2-492e-4361-a363-2b426b1b6e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "def decode_integers(integers):\n",
        "  PAD = 0\n",
        "  text = \"\"\n",
        "  for num in integers:\n",
        "    if num != PAD:\n",
        "      text +=reverse_word_index[num] + \" \"\n",
        "\n",
        "\n",
        "  return text[:-1]\n",
        "\n",
        "print(decode_integers(encoded))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftra2cx0Oy_y",
        "outputId": "eac65039-3725-44f0-858c-3aaf92490eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "that movie was just amazing so amazing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode_text(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred)\n",
        "  print(result[0])\n",
        "\n",
        "positive_review = \"That movie was awesome! I really love it and would watch it again because it was amazingly great \"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie sucked. I hated it and wouldn't watch it again . Was one of the worst thing I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSCIaBMdQhl4",
        "outputId": "358df6b1-3f72-48d4-db47-cce6c24abe77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 495ms/step\n",
            "[0.86067504]\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "[0.25067896]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Generation**"
      ],
      "metadata": {
        "id": "faZ48la9_ECN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting Shakespeare dataset"
      ],
      "metadata": {
        "id": "8_fwPJ5d-yFf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "nOY7UJTVUC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading my own dataset into google collab"
      ],
      "metadata": {
        "id": "wL7xeERJ-73M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "AkE1a2XT-wlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read , then decode for py2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding ='utf-8')\n",
        "# length of text is the number of character in it\n",
        "print ( 'Length of text : {} character'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmBJNbdaAIrk",
        "outputId": "f561e65d-c508-4dd0-ef32-57b9db5a5ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text : 1115394 character\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThEKky2BBWU7",
        "outputId": "b9e22d4b-4fb0-4dfb-b843-d141b7e51673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoding\n",
        "\n",
        "Since the text isn't encoded yet we need to encode it ourself.We are going to encode  each unique character as a different character"
      ],
      "metadata": {
        "id": "lAnGLE9-Bytu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "#Creating a mappping froom  unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "DaIbhGmPBSdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#let's look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWOacFqbDXi1",
        "outputId": "636f3487-18c2-4f17-828b-dc2c19a94a44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRUjSXtvEisC",
        "outputId": "dd7cc73a-2ba5-4844-d1e8-4e81d6453f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100 #length of  sequence for a training example\n",
        "example_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "#Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "QIDPTGv6F5NE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we can use the batch method to turn this stream of characteres into batches of desired length"
      ],
      "metadata": {
        "id": "FgIqM3bcGggg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "oBWzDy5tG8Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to use these sequences of length 101 and split them into input and output"
      ],
      "metadata": {
        "id": "Gua12EUdHjki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(chunk):\n",
        "  input_text = chunk[:1]\n",
        "  target_text = chunk[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "43PaRTMTHu9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print(\"\\n\\nEXAMPLE\\n\")\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DrLKZOzIiBJ",
        "outputId": "bdd2bf91-c19b-419a-f8c0-8f72d7927925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "F\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "a\n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "#Buffer size to shuffle the dataset\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "geqcsPIkJT1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bulding the model"
      ],
      "metadata": {
        "id": "NCrJ09J5KsUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
        "      tf.keras.layers.LSTM(rnn_units,\n",
        "                           return_sequences=True,\n",
        "                           recurrent_initializer='glorot_uniform'),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE,EMBEDDING_DIM,RNN_UNITS,BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofRVwlq1KqdY",
        "outputId": "e842c5cc-6502-4aa0-f743-d1ce612c9b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in  data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) # ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape, \"(batch_size, sequence_length, vocab_size)\") #print out the output shape"
      ],
      "metadata": {
        "id": "l1WVK8W2sivx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "965aaf18-8812-444d-c0e3-92238b16a6be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 1, 65) (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the prediction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "id": "FWMA8jkaufmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03f7ca16-6dea-405d-f9ca-f0bcf12cef20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 0.00260171  0.00094134  0.00285054 ...  0.00448469 -0.00014597\n",
            "   -0.0016118 ]]\n",
            "\n",
            " [[-0.00022803  0.00204921 -0.0009244  ...  0.00243076  0.00334891\n",
            "    0.00226893]]\n",
            "\n",
            " [[-0.00341004  0.00928213  0.00568374 ...  0.00175959  0.00089707\n",
            "    0.00137601]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.00148342  0.0058857   0.00152939 ...  0.00156662  0.00034582\n",
            "    0.00223947]]\n",
            "\n",
            " [[ 0.0013831   0.00226525 -0.0044385  ...  0.00181961 -0.00015053\n",
            "    0.0028406 ]]\n",
            "\n",
            " [[-0.00637813  0.00437854 -0.00040373 ... -0.00218587 -0.0010261\n",
            "   -0.00152669]]], shape=(64, 1, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2d array of length 100, where each interior array is the prediction for the next"
      ],
      "metadata": {
        "id": "Pnhm9ptAu11g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f9e5b2-eb78-4b24-d02d-80d154f62bf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "tf.Tensor(\n",
            "[[ 2.6017108e-03  9.4133965e-04  2.8505402e-03 -9.3627826e-04\n",
            "  -5.6448514e-03  4.1216577e-04  1.3217230e-03  3.8447573e-03\n",
            "  -3.3815112e-04 -2.7553877e-03 -5.6547774e-03  2.1169763e-03\n",
            "  -6.8874061e-03 -3.5049296e-03  3.3254190e-03 -1.1332575e-03\n",
            "   1.0287001e-03 -1.3918793e-03  4.5754132e-04  2.9779074e-03\n",
            "   2.3151322e-03  8.0871470e-03 -5.8574718e-04 -9.4085489e-04\n",
            "   2.9233717e-03 -2.5051706e-03  3.0229539e-03 -4.2441970e-04\n",
            "  -2.5586607e-03 -4.6600197e-03  2.6047085e-03 -1.7506464e-03\n",
            "  -7.2543335e-04  4.0303599e-03 -1.8295294e-03 -1.6390284e-03\n",
            "  -1.9447693e-03 -1.6999420e-03 -7.0043029e-03  3.3243257e-04\n",
            "  -1.5047812e-03 -4.5793154e-03  1.1377543e-03 -2.3780838e-03\n",
            "  -4.7986638e-03  5.8649420e-03 -5.9739044e-03  1.1070175e-03\n",
            "  -3.2567461e-03  1.3540724e-03 -1.3064244e-05  1.3104961e-03\n",
            "   2.1081115e-03  3.0668167e-04  2.0315382e-04 -2.8882492e-03\n",
            "  -2.3048026e-03  6.6770595e-03  5.7895547e-03 -5.2187522e-04\n",
            "   1.9725007e-03  4.7238925e-03  4.4846879e-03 -1.4596607e-04\n",
            "  -1.6117976e-03]], shape=(1, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probability of each character occuring next"
      ],
      "metadata": {
        "id": "SF0tS1QZwxd-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1e902f-f275-438a-f96c-dc79af6ba854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 2.6017108e-03  9.4133965e-04  2.8505402e-03 -9.3627826e-04\n",
            " -5.6448514e-03  4.1216577e-04  1.3217230e-03  3.8447573e-03\n",
            " -3.3815112e-04 -2.7553877e-03 -5.6547774e-03  2.1169763e-03\n",
            " -6.8874061e-03 -3.5049296e-03  3.3254190e-03 -1.1332575e-03\n",
            "  1.0287001e-03 -1.3918793e-03  4.5754132e-04  2.9779074e-03\n",
            "  2.3151322e-03  8.0871470e-03 -5.8574718e-04 -9.4085489e-04\n",
            "  2.9233717e-03 -2.5051706e-03  3.0229539e-03 -4.2441970e-04\n",
            " -2.5586607e-03 -4.6600197e-03  2.6047085e-03 -1.7506464e-03\n",
            " -7.2543335e-04  4.0303599e-03 -1.8295294e-03 -1.6390284e-03\n",
            " -1.9447693e-03 -1.6999420e-03 -7.0043029e-03  3.3243257e-04\n",
            " -1.5047812e-03 -4.5793154e-03  1.1377543e-03 -2.3780838e-03\n",
            " -4.7986638e-03  5.8649420e-03 -5.9739044e-03  1.1070175e-03\n",
            " -3.2567461e-03  1.3540724e-03 -1.3064244e-05  1.3104961e-03\n",
            "  2.1081115e-03  3.0668167e-04  2.0315382e-04 -2.8882492e-03\n",
            " -2.3048026e-03  6.6770595e-03  5.7895547e-03 -5.2187522e-04\n",
            "  1.9725007e-03  4.7238925e-03  4.4846879e-03 -1.4596607e-04\n",
            " -1.6117976e-03], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#If we want to determine the predicted character we need  to sample the output distribution(pick a value based on probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert  all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars"
      ],
      "metadata": {
        "id": "b3s1ixG9xN--",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4105b5f5-411a-4e47-9eb3-cec0f050c6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "';'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits,from_logits=True)"
      ],
      "metadata": {
        "id": "0AK7SHOiy7wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiling the model"
      ],
      "metadata": {
        "id": "S13bG5B7zxT5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "eu_JJ91CzwXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoint will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "#Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback= tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weight_only=True,\n",
        "    save_freq=5\n",
        ")"
      ],
      "metadata": {
        "id": "ZAtZcLytOsQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "xKH9znFCQIoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "ZDk9FTKnPin6",
        "outputId": "b871648f-a22b-4c2d-bc42-c1d1362656c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-66-eaf5a3a84a38>\", line 2, in loss  *\n        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits,from_logits=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 100) and logits.shape=(64, 1, 65)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-606d960ba999>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_file3p_dmgwr.py\u001b[0m in \u001b[0;36mtf__loss\u001b[0;34m(labels, logits)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-66-eaf5a3a84a38>\", line 2, in loss  *\n        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits,from_logits=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(64, 100) and logits.shape=(64, 1, 65)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the model"
      ],
      "metadata": {
        "id": "SrrlRHM7QLha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "gufosbo5P15T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "hEbkVFR9QQQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load  any  checkpoint we want by specifying the exact file to load"
      ],
      "metadata": {
        "id": "PobIL1i0R5QW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt\"+ str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "JJiEajzMSGlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Text**\n"
      ],
      "metadata": {
        "id": "XayY6JJFSohe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  #Evaluation step (generating text using the learned model)\n",
        "\n",
        "  #Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  #Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  #Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  #Low temperature results in more predictable text.\n",
        "  #Higher temperature results in more surprising text.\n",
        "  #Experiment to find the best setting .\n",
        "  temperature = 1.0\n",
        "\n",
        "  #Here batch size ==1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    #We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expands_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_sting + ''.join(text_generated))\n",
        "\n"
      ],
      "metadata": {
        "id": "A1f3BlCLS35e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string \")\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "id": "sjQjtqIyax8x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}